这个页面**没有“直接按 rank=100 取某一条”的单独接口/参数**；排行榜是**按页返回**的（通常 **每页 50 条**），所以要拿第 100 名你有两种常用办法：

## 方法 1：翻到第 2 页，再取第 50 条（=总榜第100）

第 1 页：1–50
第 2 页：51–100

网页第 2 页：

```text
https://www.pixiv.net/ranking.php?mode=daily&content=illust&p=2
```

如果你用 JSON（更方便解析）：

```text
https://www.pixiv.net/ranking.php?mode=daily&content=illust&p=2&format=json
```

然后在返回的 `contents` 数组里取**最后一项**（通常是第 50 条），它就是 **rank=100**。

---

## 方法 2：指定日期再取第 100（更可复现）

比如固定到 2025-12-22：

```text
https://www.pixiv.net/ranking.php?mode=daily&content=illust&date=20251222&p=2&format=json
```

同样取 `contents` 的最后一项即可。

---

如果你把你抓到的 `format=json` 返回（或者你要我按今天最新的日榜），我也可以直接帮你定位第 100 名对应的 `illust_id`。




因为 pixiv 这类站通常有两套“取数据”的路子：

1. **页面自身的后门/历史接口**（给排行榜、搜索这种“列表页”用）
2. **ajax 的业务接口**（给作品详情、用户作品、关注流这种“模块”用）

你一直用的这个：

`/ajax/user/1878082/works/latest?lang=zh`

它天生就是“**某个用户的最新作品列表**”，所以它的返回结构是 `body.illusts{ id: {...}, ... }`，并且还夹了一堆 `null`（通常是为了分页/占位或按某种策略裁剪字段）。它不可能给你“全站日榜排行”。

---

## 我怎么知道 `ranking.php?format=json` 这条路的？

核心就三步（本质是“从页面反推数据源”）：

### 1) 先确认：排行榜数据确实已经在页面里出现过

你前面贴过 `__NEXT_DATA__`，里面直接就有：

* `props.pageProps.assign.contents`（一整页排行数据）
* `rank_total: 500`
* `page`, `next`, `prev`, `date`, `mode` 等

这说明：**服务端生成排行榜页时，手上已经拿到了“结构化的榜单数据”**，只是在 HTML 里塞给前端而已。

### 2) 再确认：这类“列表页”经常有 `format=json` 这种 legacy 参数

很多站会保留一个“同一路径、换个参数就吐 JSON”的形式：

* 方便旧版页面/抓取/内部调试
* 同时也能给前端 SSR/缓存层复用

pixiv 的排行榜正好就是这种：`ranking.php` 加 `format=json` 会直接返回榜单 JSON。

### 3) 用你已经看到的字段去验证

比如你在 `__NEXT_DATA__` 里看到的字段（`contents[]` 每项的 `illust_id/rank/view_count/rating_count/url`），和 `format=json` 出来的字段能对上，就证明这条路是“正经数据源/等价数据源”。

---

## “它能返回全部排行”这句话怎么理解更准确？

它不是一次性把 500 条全吐给你，而是：

* **每页 50 条**
* 总数通常 `rank_total = 500`
* 你通过 `p=1..10` 把 500 条拼起来

也就是：

* 第 1 页：1–50
* 第 2 页：51–100
* …
* 第 10 页：451–500

所以你要“全榜”，就是循环请求 10 页。

---

## 你现在这两类接口怎么选用？

### 拿“排行榜（全站榜单）”

用：

* `https://www.pixiv.net/ranking.php?...&format=json`

### 拿“某个用户的作品流”

用：

* `/ajax/user/{userId}/works/latest`
* 或者其他 `/ajax/user/.../profile/all` / `/ajax/user/.../illusts` 之类（看你需求）

它们解决的问题不一样：一个是“榜单聚合”，一个是“用户维度”。

---

如果你说下你最终目标是：**只要某一天日榜第 N 名的 illust_id**，还是要**批量拉 500 条并入库**，我可以给你一个最省请求数、最稳的抓取策略（包括并发、重试、避免被风控的 header/cookie 用法等）。

